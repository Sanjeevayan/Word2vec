{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kumarsanjeev/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# New Request Offering: \"Request for an Equipment i.e. laptop\"\n",
    "\n",
    "# Using Gensim library & word2vec model(one of the most state of the art algo to find semantic similarity of terms)\n",
    "# Word2vec is one algorithm which uses shallow neural network for learning a word embedding from a text corpus.\n",
    "# This word2vec model has been trained on Google news data\n",
    "# It includes word vectors for a vocabulary of 3 million words and phrases \n",
    "# they were trained on roughly 100 billion words from a Google News dataset. The vector length is 300.\n",
    "\n",
    "import gensim\n",
    "\n",
    "#model = gensim.models.Word2Vec.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/kumarsanjeev/Downloads/GoogleNews-vectors-negative300.bin\",binary = True)\n",
    "#print(model.similar_by_word('laptop',topn=20))\n",
    "\n",
    "#print(model.similar_by_word('want' ,topn=20))\n",
    "# print(model.similar_by_word('projector' ,topn=20))\n",
    "#print(model.similar_by_word('laptop_computers' ,topn=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an entity:computer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kumarsanjeev/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.6473335027694702), ('com_puter', 0.6082079410552979), ('technician_Leonard_Luchko', 0.566274881362915), ('mainframes_minicomputers', 0.5617720484733582), ('laptop_computers', 0.5585449934005737), ('PC', 0.5539618730545044), ('maker_Dell_DELL.O', 0.5519253611564636), ('laptops', 0.5517531633377075), ('PCs', 0.5517113208770752), ('Computers', 0.5511513948440552), ('Los_Alamos_Natl', 0.5500181913375854), ('software', 0.5444109439849854), ('logs_keystrokes', 0.5432291626930237), ('desktop_computers', 0.5428782105445862), ('view_HealthCast', 0.5407893061637878), ('Surface_tabletop', 0.5396082997322083), ('Evi_Susilowati', 0.5348566770553589)]\n"
     ]
    }
   ],
   "source": [
    "# Automatic sentence generation using Entity & Taxonomy terms  through Word2vec model\n",
    "sent = \"I want a new laptop\".split()# summary/description provided by User\n",
    "entity = input (\"Enter an entity:\")\n",
    "# Entity & Taxonomy terms  Extraction through Word2vec model\n",
    "entity_list = model.wv.similar_by_word(entity ,topn=20)# word2vec Model extracts all the related terms (entity & taxonomy terms) in same semantic vector space.\n",
    "print (entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would like to have new com_puter .\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-41f40d919166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Storing the output in a text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' \\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/kumarsanjeev/Desktop/Test5.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/kumarsanjeev/Desktop/Test5.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for _ in range(5):\n",
    "    subj = [\"I\"]\n",
    "    verb = [\"want\",\"need\",\"want to have\",\"would like to have\"]# Pattern words: User will have to provide keywords for the verbs to dentify the patterns. \n",
    "    adj = [\"new\"]\n",
    "    obj = []#adds all the  entities and taxonomy terms extracted by word2vec Model\n",
    "    for i in entity_list:\n",
    "        obj.append(i[0])\n",
    "\n",
    "    l=[subj,verb,adj,obj,\".\"] \n",
    "    m = ' '.join([random.choice(i) for i in l])\n",
    "    print (m)\n",
    "\n",
    "    # Storing the output in a text file\n",
    "    s = s + m + ' \\n'\n",
    "    output_file = open(\"/Users/kumarsanjeev/Desktop/Test5.txt\")\n",
    "    path = \"/Users/kumarsanjeev/Desktop/Test5.txt\"\n",
    "    with open(path , 'w') as file:\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an entity:laptop\n",
      "[('laptops', 0.805374026298523), ('laptop_computer', 0.7848465442657471), ('notebook', 0.67857825756073), ('netbook', 0.6707929372787476), ('computer', 0.6640493273735046), ('laptop_computers', 0.6633790731430054), ('notebook_PC', 0.6631842851638794), ('MacBook', 0.6598750352859497), ('PowerBook', 0.6520565152168274), ('Sony_Vaio_laptop', 0.6496157646179199), ('notebooks', 0.6491786241531372), ('Macbook', 0.6486797332763672), ('Laptop', 0.648013710975647), ('IBM_ThinkPad_T##', 0.6193387508392334), ('MacBook_Pro', 0.6187069416046143), ('Dell_Latitude_D###', 0.6164904832839966), ('Macbook_Pro', 0.6115914583206177), ('computers', 0.6102420091629028), ('Dell_Inspiron_laptop', 0.6090463399887085), ('notebook_computers', 0.607465386390686)]\n"
     ]
    }
   ],
   "source": [
    "# Automatic sentence generation using Entity & Taxonomy terms  through Word2vec model\n",
    "#sent = \"I want a new laptop\".split()\n",
    "entity = input (\"Enter an entity:\")\n",
    "\n",
    "list1 = model.similar_by_word(entity ,topn=20)\n",
    "print (list1)\n",
    "# list2 = model.similar_by_word('want', topn=20)\n",
    "# for i, j in zip(list1,list2):\n",
    "#     print(\"i {} a {}\".format(j[0] ,i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity & Taxonomy terms  Extraction through Word2vec model\n",
    "import random\n",
    "#sent = \"I want a laptop\".split()\n",
    "list1 = model.similar_by_word('laptop' ,topn=20)\n",
    "#list2 = model.similar_by_word('want', topn=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to have IBM_ThinkPad_T## .\n"
     ]
    }
   ],
   "source": [
    "# Sentence Generation for the  pattern : Subject + Verb + Object\n",
    "subj = [\"I\"]\n",
    "verb = [\"want\",\"need\",\"want to have\",\"would like to have\"]# User will have tp provide keywords for the verbs to dentify the patterns. \n",
    "obj = []\n",
    "for i in list1:\n",
    "    obj.append(i[0])\n",
    "\n",
    "l=[subj,verb,obj,\".\"] \n",
    "m = ' '.join([random.choice(i) for i in l])\n",
    "print (m)\n",
    "\n",
    "# Storing the output in a text file\n",
    "s = s + m + ' \\n'\n",
    "output_file = open(\"/Users/kumarsanjeev/Desktop/Test5.txt\")\n",
    "path = \"/Users/kumarsanjeev/Desktop/Test5.txt\"\n",
    "with open(path , 'w') as file:\n",
    "    file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the output in a text file\n",
    "s = s + m + ' \\n'\n",
    "output_file = open(\"/Users/kumarsanjeev/Desktop/Test5.txt\")\n",
    "path = \"/Users/kumarsanjeev/Desktop/Test5.txt\"\n",
    "with open(path , 'w') as file:\n",
    "    file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a pronoun:I\n",
      "enter a pronoun:We\n",
      "enter a pronoun:They\n",
      "['I', 'We', 'They']\n"
     ]
    }
   ],
   "source": [
    "# User Input for subject\n",
    "subj = []\n",
    "for i in range(3):\n",
    "    subject = input(\"enter a pronoun:\")\n",
    "    subj.append(subject)\n",
    "\n",
    "    print (subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an entity:equipment\n",
      "[('equipments', 0.7184020280838013), ('Equipment', 0.6507868766784668), ('equiment', 0.64060378074646), ('equip_ment', 0.6304388046264648), ('equiptment', 0.6143404245376587), ('machinery', 0.59406578540802), ('equipement', 0.5912227630615234), ('eqipment', 0.5761674642562866), ('cranes_forklifts', 0.5646820664405823), ('upgrade_Fortkamp', 0.5597618222236633), ('wooden_sarcophagus_containing', 0.5492346286773682), ('gear', 0.5489619374275208), ('equpment', 0.546511173248291), ('manager_Rob_Cucuzza', 0.533706784248352), ('professionals_ELFA_SmartBrief', 0.518406867980957), ('manager_Dwayne_Mandrusiak', 0.5170284509658813), ('hoses_nozzles', 0.5133376121520996), ('digger_derricks', 0.5107513666152954), ('compressors_generators', 0.5076912641525269), ('Dana_Heinze', 0.5019118785858154)]\n"
     ]
    }
   ],
   "source": [
    "# User Input for entity\n",
    "sent = \"I want a new laptop\".split()\n",
    "entity = input (\"Enter an entity:\")\n",
    "\n",
    "\n",
    "list1 = model.similar_by_word(entity ,topn=20)\n",
    "print (list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next week DATE\n",
      "Madrid GPE\n",
      "SAP ORG\n"
     ]
    }
   ],
   "source": [
    "#Spacy NER\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(\"\"\"\"Next week I'll be in Madrid.I want a new computer.I want to loan a projector.I need to reset my domain password.I need to restore my desktop.I need to have my account unlocked.I want to take backup of files.I need an email alias.I would like to have an email account.I want to have account for SAP.\"\"\")\n",
    "# for line in doc:\n",
    "#     print (line)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : Mark, , Entity : PERSON\n",
      "Word : Elliot, , Entity : PERSON\n",
      "Word : Zuckerberg, , Entity : PERSON\n",
      "Word : (, , Entity : \n",
      "Word : born, , Entity : \n",
      "Word : May, , Entity : DATE\n",
      "Word : 14, , Entity : DATE\n",
      "Word : ,, , Entity : DATE\n",
      "Word : 1984, , Entity : DATE\n",
      "Word : ), , Entity : \n",
      "Word : is, , Entity : \n",
      "Word : a, , Entity : \n",
      "Word : co, , Entity : \n",
      "Word : -, , Entity : \n",
      "Word : founder, , Entity : \n",
      "Word : of, , Entity : \n",
      "Word : Facebook, , Entity : ORG\n",
      "Word : ., , Entity : \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_fr = spacy.load(\"en\")\n",
    "\n",
    "# We create a sentence\n",
    "text_en = \"Mark Elliot Zuckerberg (born May 14, 1984) is a co-founder of Facebook.\"\n",
    "\n",
    "# We process the sentence through the pipeline\n",
    "doc_en = nlp_fr(text_en)\n",
    "\n",
    "# We print each token and if an entity is recognized, we print the entity type\n",
    "for token in doc_en:\n",
    "#     print (token.text,token.ent_type_)\n",
    "    print('Word : {0}, , Entity : {1}' .format(token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
